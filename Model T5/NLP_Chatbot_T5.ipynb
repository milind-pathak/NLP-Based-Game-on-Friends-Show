{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hmnXcSEFMK63"
      },
      "outputs": [],
      "source": [
        "#!pip install transformers\n",
        "#!pip install sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "from sklearn.metrics import mean_squared_error, accuracy_score\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "jxl4y3l4MhxM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "dataset = pd.read_csv('/content/Trial-Dataset.csv')"
      ],
      "metadata": {
        "id": "VtR8ZpL8Mhzv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PlayerInputOutputDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer, source_max_token_len=128, target_max_token_len=128):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = data\n",
        "        self.source_max_token_len = source_max_token_len\n",
        "        self.target_max_token_len = target_max_token_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        data_row = self.data.iloc[index]\n",
        "\n",
        "        source_encoding = tokenizer(\n",
        "            data_row['Player_Input'],\n",
        "            max_length=self.source_max_token_len,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            add_special_tokens=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        target_encoding = tokenizer(\n",
        "            data_row['Output'],\n",
        "            max_length=self.target_max_token_len,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            add_special_tokens=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        labels = target_encoding['input_ids']\n",
        "        labels[labels == 0] = -100\n",
        "\n",
        "        return dict(\n",
        "            input_ids=source_encoding['input_ids'].flatten(),\n",
        "            attention_mask=source_encoding['attention_mask'].flatten(),\n",
        "            labels=labels.flatten()\n",
        "        )\n"
      ],
      "metadata": {
        "id": "qUYFK8yJMh23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 't5-small'\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "train_df, val_df = train_test_split(dataset, test_size=0.1)\n",
        "\n",
        "train_dataset = PlayerInputOutputDataset(train_df, tokenizer)\n",
        "val_dataset = PlayerInputOutputDataset(val_df, tokenizer)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=8)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4UacUE8_Mh6u",
        "outputId": "f3c5b820-bdbe-40ef-f2a1-a8f0c2fd1da8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n"
      ],
      "metadata": {
        "id": "d30NKdcVMh8G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 20\n",
        "learning_rate = 3e-4\n",
        "adam_epsilon = 1e-8\n",
        "total_steps = len(train_loader) * num_epochs\n",
        "model_path = \"/content/t5_fine_tuned\"\n",
        "\n",
        "early_stopping_patience = 3\n",
        "early_stopping_counter = 0\n",
        "best_val_loss = float('inf')\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=learning_rate, eps=adam_epsilon)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f'Epoch {epoch + 1}/{num_epochs}')\n",
        "    print('-' * 10)\n",
        "\n",
        "    model.train()\n",
        "    train_losses = []\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        train_losses.append(loss.item())\n",
        "\n",
        "    train_loss = np.mean(train_losses)\n",
        "    print(f'Train loss {train_loss}')\n",
        "\n",
        "    model.eval()\n",
        "    val_losses = []\n",
        "    for batch in val_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "\n",
        "        loss = outputs.loss\n",
        "        val_losses.append(loss.item())\n",
        "\n",
        "    val_loss = np.mean(val_losses)\n",
        "    print(f'Validation loss {val_loss}')\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        model.save_pretrained(model_path)\n",
        "        tokenizer.save_pretrained(model_path)\n",
        "        early_stopping_counter = 0\n",
        "    else:\n",
        "        early_stopping_counter += 1\n",
        "        if early_stopping_counter >= early_stopping_patience:\n",
        "            print('Early stopping triggered.')\n",
        "            break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A1yXunF0P9mJ",
        "outputId": "24f4ce67-7c4c-49ea-b9a8-8fce7e715e6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "----------\n",
            "Train loss 4.783982515335083\n",
            "Validation loss 3.2447972297668457\n",
            "Epoch 2/20\n",
            "----------\n",
            "Train loss 3.5266423333774912\n",
            "Validation loss 2.7534426053365073\n",
            "Epoch 3/20\n",
            "----------\n",
            "Train loss 3.0623910535465586\n",
            "Validation loss 2.474454085032145\n",
            "Epoch 4/20\n",
            "----------\n",
            "Train loss 2.7003535249016504\n",
            "Validation loss 2.1779896020889282\n",
            "Epoch 5/20\n",
            "----------\n",
            "Train loss 2.408356612378901\n",
            "Validation loss 1.9691200256347656\n",
            "Epoch 6/20\n",
            "----------\n",
            "Train loss 2.1932009133425625\n",
            "Validation loss 1.7815261681874592\n",
            "Epoch 7/20\n",
            "----------\n",
            "Train loss 1.9958446513522754\n",
            "Validation loss 1.6047706604003906\n",
            "Epoch 8/20\n",
            "----------\n",
            "Train loss 1.8286893313581294\n",
            "Validation loss 1.4497205813725789\n",
            "Epoch 9/20\n",
            "----------\n",
            "Train loss 1.675434486432509\n",
            "Validation loss 1.3368101914723713\n",
            "Epoch 10/20\n",
            "----------\n",
            "Train loss 1.539536096832969\n",
            "Validation loss 1.2284695307413738\n",
            "Epoch 11/20\n",
            "----------\n",
            "Train loss 1.4320017261938616\n",
            "Validation loss 1.1178052425384521\n",
            "Epoch 12/20\n",
            "----------\n",
            "Train loss 1.3361592482436786\n",
            "Validation loss 1.0272028644879658\n",
            "Epoch 13/20\n",
            "----------\n",
            "Train loss 1.2308289300311694\n",
            "Validation loss 0.9439565340677897\n",
            "Epoch 14/20\n",
            "----------\n",
            "Train loss 1.1755190383304248\n",
            "Validation loss 0.8889104525248209\n",
            "Epoch 15/20\n",
            "----------\n",
            "Train loss 1.1072626195170663\n",
            "Validation loss 0.8483731547991434\n",
            "Epoch 16/20\n",
            "----------\n",
            "Train loss 1.053643142635172\n",
            "Validation loss 0.804502546787262\n",
            "Epoch 17/20\n",
            "----------\n",
            "Train loss 1.0362338775938207\n",
            "Validation loss 0.7797091007232666\n",
            "Epoch 18/20\n",
            "----------\n",
            "Train loss 0.9867922446944497\n",
            "Validation loss 0.7545019388198853\n",
            "Epoch 19/20\n",
            "----------\n",
            "Train loss 0.9967641559514132\n",
            "Validation loss 0.7419907649358114\n",
            "Epoch 20/20\n",
            "----------\n",
            "Train loss 0.9786240648139607\n",
            "Validation loss 0.7365411321322123\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model_path = \"/content/t5_fine_tuned\"\n",
        "model.save_pretrained(model_path)\n",
        "tokenizer.save_pretrained(model_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W7bSzZx-Mh-N",
        "outputId": "e17a1d44-633a-45fe-85f1-3ac219b5f612"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/t5_fine_tuned/tokenizer_config.json',\n",
              " '/content/t5_fine_tuned/special_tokens_map.json',\n",
              " '/content/t5_fine_tuned/spiece.model',\n",
              " '/content/t5_fine_tuned/added_tokens.json')"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = T5ForConditionalGeneration.from_pretrained('/content/t5_fine_tuned').to(device)\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "input_texts = [\"order a coffee\", \"tell a joke\", \"jump around\", \"turn off the alarm\", \"Hey Monica, How are you?\"]\n",
        "\n",
        "for input_text in input_texts:\n",
        "    input_ids = tokenizer(input_text, return_tensors='pt').input_ids.to(device)\n",
        "    outputs = model.generate(input_ids)\n",
        "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    print(f\"Input: {input_text}\\nOutput: {generated_text}\\n\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ITGb9XbMiAl",
        "outputId": "ca11dc84-1ad9-4c9c-c535-2077df6bc0aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: order a coffee\n",
            "Output: Chandler's eyes light up as he orders a cup of the day's special\n",
            "\n",
            "Input: tell a joke\n",
            "Output: Chandler chuckles at the news, a joke that's a joke about nature\n",
            "\n",
            "Input: jump around\n",
            "Output: \"Chandler's face shows the strain of a swimming\n",
            "\n",
            "Input: turn off the alarm\n",
            "Output: \"Chandler unwraps the alarm, prompting an alarm bell.\" Monica:\n",
            "\n",
            "Input: Hey Monica, How are you?\n",
            "Output: \"Monica:\"Oh, how are you?\"\n",
            "\n"
          ]
        }
      ]
    }
  ]
}